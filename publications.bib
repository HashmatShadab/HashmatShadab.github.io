
@misc{malik_adversarial_2022,
	title = {Adversarial {Pixel} {Restoration} as a {Pretext} {Task} for {Transferable} {Perturbations}},
	url = {http://arxiv.org/abs/2207.08803},
	doi = {10.48550/arXiv.2207.08803},
	abstract = {Transferable adversarial attacks optimize adversaries from a pretrained surrogate model and known label space to fool the unknown black-box models. Therefore, these attacks are restricted by the availability of an effective surrogate model. In this work, we relax this assumption and propose Adversarial Pixel Restoration as a self-supervised alternative to train an effective surrogate model from scratch under the condition of no labels and few data samples. Our training approach is based on a min-max scheme which reduces overfitting via an adversarial objective and thus optimizes for a more generalizable surrogate model. Our proposed attack is complimentary to the adversarial pixel restoration and is independent of any task specific objective as it can be launched in a self-supervised manner. We successfully demonstrate the adversarial transferability of our approach to Vision Transformers as well as Convolutional Neural Networks for the tasks of classification, object detection, and video segmentation. Our training approach improves the transferability of the baseline unsupervised training method by 16.4\% on ImageNet val. set. Our codes \& pre-trained surrogate models are available at: https://github.com/HashmatShadab/APR},
	urldate = {2024-10-13},
	publisher = {arXiv},
	author = {Malik, Hashmat Shadab and Kunhimon, Shahina K. and Naseer, Muzammal and Khan, Salman and Khan, Fahad Shahbaz},
	month = oct,
	year = {2022},
	note = {arXiv:2207.08803},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\97152\\Zotero\\storage\\EWTXG529\\Malik et al. - 2022 - Adversarial Pixel Restoration as a Pretext Task fo.pdf:application/pdf;Snapshot:C\:\\Users\\97152\\Zotero\\storage\\6MK2GLWA\\2207.html:text/html},
}

@misc{malik_towards_2024,
	title = {Towards {Evaluating} the {Robustness} of {Visual} {State} {Space} {Models}},
	url = {http://arxiv.org/abs/2406.09407},
	doi = {10.48550/arXiv.2406.09407},
	abstract = {Vision State Space Models (VSSMs), a novel architecture that combines the strengths of recurrent neural networks and latent variable models, have demonstrated remarkable performance in visual perception tasks by efficiently capturing long-range dependencies and modeling complex visual dynamics. However, their robustness under natural and adversarial perturbations remains a critical concern. In this work, we present a comprehensive evaluation of VSSMs' robustness under various perturbation scenarios, including occlusions, image structure, common corruptions, and adversarial attacks, and compare their performance to well-established architectures such as transformers and Convolutional Neural Networks. Furthermore, we investigate the resilience of VSSMs to object-background compositional changes on sophisticated benchmarks designed to test model performance in complex visual scenes. We also assess their robustness on object detection and segmentation tasks using corrupted datasets that mimic real-world scenarios. To gain a deeper understanding of VSSMs' adversarial robustness, we conduct a frequency-based analysis of adversarial attacks, evaluating their performance against low-frequency and high-frequency perturbations. Our findings highlight the strengths and limitations of VSSMs in handling complex visual corruptions, offering valuable insights for future research. Our code and models will be available at https://github.com/HashmatShadab/MambaRobustness.},
	urldate = {2024-10-13},
	publisher = {arXiv},
	author = {Malik, Hashmat Shadab and Shamshad, Fahad and Naseer, Muzammal and Nandakumar, Karthik and Khan, Fahad Shahbaz and Khan, Salman},
	month = sep,
	year = {2024},
	note = {arXiv:2406.09407},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\97152\\Zotero\\storage\\XPV7CNWF\\Malik et al. - 2024 - Towards Evaluating the Robustness of Visual State .pdf:application/pdf;Snapshot:C\:\\Users\\97152\\Zotero\\storage\\6DB6ZSLS\\2406.html:text/html},
}

@misc{malik_objectcompose_2024,
	title = {{ObjectCompose}: {Evaluating} {Resilience} of {Vision}-{Based} {Models} on {Object}-to-{Background} {Compositional} {Changes}},
	shorttitle = {{ObjectCompose}},
	url = {http://arxiv.org/abs/2403.04701},
	doi = {10.48550/arXiv.2403.04701},
	abstract = {Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse object-to-background changes while preserving the original semantics and appearance of the object. To achieve this goal, we harness the generative capabilities of text-to-image, image-to-text, and image-to-segment models to automatically generate a broad spectrum of object-to-background changes. We induce both natural and adversarial background changes by either modifying the textual prompts or optimizing the latents and textual embedding of text-to-image models. We produce various versions of standard vision datasets (ImageNet, COCO), incorporating either diverse and realistic backgrounds into the images or introducing color, texture, and adversarial changes in the background. We conduct extensive experiments to analyze the robustness of vision-based models against object-to-background context variations across diverse tasks. Code https://github.com/Muhammad-Huzaifaa/ObjectCompose.},
	urldate = {2024-10-13},
	publisher = {arXiv},
	author = {Malik, Hashmat Shadab and Huzaifa, Muhammad and Naseer, Muzammal and Khan, Salman and Khan, Fahad Shahbaz},
	month = oct,
	year = {2024},
	note = {arXiv:2403.04701},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\97152\\Zotero\\storage\\C5XDADSN\\Malik et al. - 2024 - ObjectCompose Evaluating Resilience of Vision-Bas.pdf:application/pdf;Snapshot:C\:\\Users\\97152\\Zotero\\storage\\V9Y7WXP6\\2403.html:text/html},
}

@misc{malik_evaluating_2024,
	title = {On {Evaluating} {Adversarial} {Robustness} of {Volumetric} {Medical} {Segmentation} {Models}},
	url = {http://arxiv.org/abs/2406.08486},
	doi = {10.48550/arXiv.2406.08486},
	abstract = {Volumetric medical segmentation models have achieved significant success on organ and tumor-based segmentation tasks in recent years. However, their vulnerability to adversarial attacks remains largely unexplored, raising serious concerns regarding the real-world deployment of tools employing such models in the healthcare sector. This underscores the importance of investigating the robustness of existing models. In this context, our work aims to empirically examine the adversarial robustness across current volumetric segmentation architectures, encompassing Convolutional, Transformer, and Mamba-based models. We extend this investigation across four volumetric segmentation datasets, evaluating robustness under both white box and black box adversarial attacks. Overall, we observe that while both pixel and frequency-based attacks perform reasonably well under {\textbackslash}emph\{white box\} setting, the latter performs significantly better under transfer-based black box attacks. Across our experiments, we observe transformer-based models show higher robustness than convolution-based models with Mamba-based models being the most vulnerable. Additionally, we show that large-scale training of volumetric segmentation models improves the model's robustness against adversarial attacks. The code and robust models are available at https://github.com/HashmatShadab/Robustness-of-Volumetric-Medical-Segmentation-Models.},
	urldate = {2024-10-13},
	publisher = {arXiv},
	author = {Malik, Hashmat Shadab and Saeed, Numan and Hanif, Asif and Naseer, Muzammal and Yaqub, Mohammad and Khan, Salman and Khan, Fahad Shahbaz},
	month = sep,
	year = {2024},
	note = {arXiv:2406.08486},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}
